# CellSem LLM Client

[![Tests](https://github.com/Cellular-Semantics/cellsem_llm_client/actions/workflows/test.yml/badge.svg?branch=main)](https://github.com/Cellular-Semantics/cellsem_llm_client/actions/workflows/test.yml)
[![coverage](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/Cellular-Semantics/cellsem_llm_client/main/.github/badges/coverage.json)](https://github.com/Cellular-Semantics/cellsem_llm_client/actions/workflows/test.yml)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)

A flexible LLM client with multi-provider support, built using LiteLLM + Pydantic for seamless integration across OpenAI, Anthropic, and other providers.

## üöÄ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/Cellular-Semantics/cellsem_llm_client.git
cd cellsem_llm_client

# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create environment and install dependencies
uv sync --dev

# Set up pre-commit hooks (optional but recommended)
uv run pre-commit install
```

### Environment Setup

Create a `.env` file in the project root:

```bash
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here
```

### Basic Usage

```python
from cellsem_llm_client import create_openai_agent, create_anthropic_agent

# Create agents with automatic environment configuration
openai_agent = create_openai_agent()
anthropic_agent = create_anthropic_agent()

# Simple queries
openai_response = openai_agent.query("Explain quantum computing in 50 words")
claude_response = anthropic_agent.query("What are the benefits of renewable energy?")

# Custom configuration
custom_agent = create_openai_agent(
    model="gpt-4",
    max_tokens=2000
)
```



## üìö Documentation

**Full Documentation**: [https://cellular-semantics.github.io/cellsem_llm_client/](https://cellular-semantics.github.io/cellsem_llm_client/)

The documentation is built automatically from the `docs/` folder on each push to main using GitHub Actions.

Quick links:
- [Installation Guide](docs/installation.md)
- [Quick Start Tutorial](docs/quickstart.md)
- [Development Guidelines](docs/contributing.md)
- [API Reference](docs/api/cellsem_llm_client/index.rst) (auto-generated)
- [Schema Enforcement](docs/schema_enforcement.md)

## ‚ú® Current Features

### LLM client

STATUS - beta

- ‚úÖ **Multi-Provider Support**: Seamless switching between OpenAI, Anthropic, and other LiteLLM-supported providers
- ‚úÖ **Environment-Based Configuration**: Automatic API key loading from environment variables
- ‚úÖ **Type Safety**: Full MyPy type checking with strict configuration
- ‚úÖ **Abstract Base Classes**: Clean architecture with provider-specific implementations
- ‚úÖ **Error Handling**: Robust validation and error management
- ‚úÖ **Configuration Utilities**: Helper functions for quick agent setup

###  Token Tracking & Cost Monitoring

STATUS - beta

- ‚úÖ **Real-time Cost Tracking**: Direct integration with OpenAI and Anthropic usage APIs (aggregate per-key)
- ‚úÖ **Token Usage Metrics**: Detailed tracking of input, output, cached, and thinking tokens
- ‚úÖ **Cost Calculation**: Automated cost computation with fallback rate database (per-request precision)
- ‚úÖ **Usage Analytics**: Comprehensive reporting and cost optimization insights

### JSON Schema Compliance

STATUS - beta

- ‚úÖ **Native Schema Support**: OpenAI structured outputs with `strict=true` enforcement
- ‚úÖ **Tool Use Integration**: Anthropic schema validation via tool use patterns
- ‚úÖ **Pydantic Integration**: Automatic model validation and retry logic
- ‚úÖ **Cross-Provider Compatibility**: Unified schema interface across all providers
- ‚úÖ **JSON-First Inputs**: Prefer plain JSON Schema dicts; Pydantic models are optional helpers

### Tool Calling & Ontology Search

STATUS - alpha

- ‚úÖ **LiteLLM Tool Loop**: `LiteLLMAgent.query_with_tools` executes tool calls and resumes the conversation
- ‚úÖ **OLS4 MCP Tool**: Built-in `ols4_search` helper targeting the EBI OLS4 MCP (with legacy fallback) for ontology lookups
- ‚úÖ **Integration Coverage**: Live test hits OLS4 for ‚ÄúBergmann glial cell‚Äù to verify real responses
- ‚úÖ **Composable**: Tool definitions + handlers returned together for easy plug-in to agents

### Schema Enforcement (JSON-first)

STATUS - beta

- üéØ **Use JSON Schema Directly**: Pass a JSON Schema `dict` to enforce structure; optionally derive it from a Pydantic model via `model_json_schema()` or by schema name (resolves `<name>.json` in your schema directory).
- üîå **Provider-Aware**: OpenAI uses strict `response_format`; Anthropic converts the schema to a single enforced tool call; other providers get prompt hints plus post-validation.
- üîÅ **Validate & Retry**: Responses are parsed and validated against the derived Pydantic model with lightweight retries; hard failures raise `SchemaValidationException`.
- üß∞ **Runtime Model Generation**: `SchemaManager` loads schemas from dict/file/URL and generates/caches Pydantic models on the fly.
- ü™Ñ **Example (JSON-first)**:
  ```python
  from cellsem_llm_client.agents import LiteLLMAgent

  schema = {
      "type": "object",
      "properties": {
          "term": {"type": "string", "description": "Cell type name"},
          "iri": {"type": "string", "format": "uri"},
      },
      "required": ["term", "iri"],
      "additionalProperties": False,
  }

  agent = LiteLLMAgent(model="gpt-4o", api_key="your-key")
  result = agent.query_with_schema(
      message="Return a cell type name and IRI.",
      schema=schema,  # JSON-first
  )
  print(result.model_dump())  # Pydantic model generated at runtime
  ```

## Planned/Under developemnt

###  File Attachment Support
- ‚è≥ **Multi-Format Support**: Images (PNG, JPEG, WebP), PDFs, and documents
- ‚è≥ **Provider Abstraction**: Unified file API across different LLM providers
- ‚è≥ **Capability Detection**: Automatic model file support validation
- ‚è≥ **Flexible Input**: Base64, URL, and file path support

### AI-Powered Model Recommendations
- ‚è≥ **Task Complexity Analysis**: AI-powered prompt difficulty assessment
- ‚è≥ **Model Selection**: Intelligent recommendations based on task requirements
- ‚è≥ **Cost Optimization**: Balance performance and cost for optimal model choice
- ‚è≥ **Token Estimation**: Predict token usage for better planning

## üèóÔ∏è Architecture

```
cellsem_llm_client/
‚îú‚îÄ‚îÄ agents/          # Core LLM agent implementations
‚îú‚îÄ‚îÄ utils/           # Configuration and helper utilities
‚îú‚îÄ‚îÄ tracking/        # Token usage and cost monitoring 
‚îú‚îÄ‚îÄ schema/          # JSON schema validation and compliance 
‚îú‚îÄ‚îÄ files/           # File attachment processing (Stub)
‚îî‚îÄ‚îÄ advisors/        # AI-powered model recommendations (Stub)
```

## üìã Requirements

- **Python**: 3.11+
- **Dependencies**: LiteLLM, Pydantic, python-dotenv
- **API Keys**: OpenAI and/or Anthropic API keys for full functionality

## ü§ù Contributing

1. Follow the TDD workflow defined in `CLAUDE.md`
2. Write tests first, implement features to pass tests
3. Ensure all quality checks pass: `ruff`, `mypy`, `pytest`
4. Maintain >85% test coverage
5. Use conventional commit messages

See [`planning/ROADMAP.md`](planning/ROADMAP.md) for detailed implementation plans for pending features.

### üß™ Testing Strategy

- **Unit Tests**: Fast, isolated tests with mocked dependencies
- **Integration Tests**: Real API validation in development, controlled mocks in CI
- **Environment-Based**: `USE_MOCKS=true` for CI, real APIs for local development
- **Coverage**: >90% code coverage maintained across all modules

### Development Workflow

```bash
# Run tests
uv run pytest                    # All tests
uv run pytest -m unit           # Unit tests only
uv run pytest -m integration    # Integration tests only

# Code quality checks
uv run ruff check --fix src/ tests/   # Lint and auto-fix
uv run ruff format src/ tests/        # Format code
uv run mypy src/                      # Type checking

# Run with coverage
uv run pytest --cov=cellsem_llm_client --cov-report=html
```

## üìÑ License

MIT License - see LICENSE file for details.
